{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2112eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "An artificial neuron, also known as a perceptron, is a basic unit of an artificial neural network (ANN). It is similar to a biological neuron in that it receives input, processes it, and produces output. The main components of an artificial neuron are:\n",
    "\n",
    "Inputs: These are the signals received from other neurons or the external environment.\n",
    "Weights: Each input signal is multiplied by a weight, which determines the importance of that input to the neuron's output.\n",
    "Bias: This is a constant value that is added to the weighted sum of inputs to shift the activation function.\n",
    "Activation function: This function takes the weighted sum of inputs and bias as input and produces an output. It is typically a non-linear function that introduces non-linearity to the neuron's output.\n",
    "Output: This is the final output of the neuron, which is sent to other neurons in the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The different types of activation functions popularly used in ANNs are:\n",
    "\n",
    "Sigmoid function: This function produces an output between 0 and 1, and is used in binary classification problems. The output of the sigmoid function can be interpreted as the probability of a particular class.\n",
    "ReLU function: This function produces an output of 0 for negative input values and the input value for positive input values. It is commonly used in deep neural networks and helps to overcome the vanishing gradient problem.\n",
    "Tanh function: This function produces an output between -1 and 1, and is used in classification problems where the output can be negative or positive.\n",
    "Softmax function: This function produces an output between 0 and 1 for each class in a multi-class classification problem. The outputs of the softmax function can be interpreted as the probability of each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### Explain the below statements ?\n",
    "1.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "2.\tUse a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c161e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the weights given, we can compute the output for each data point as follows:\n",
    "\n",
    "For (3, 4):\n",
    "Output = w0 + w1 * 3 + w2 * 4 = -1 + 2 * 3 + 1 * 4 = 8\n",
    "\n",
    "For (5, 2):\n",
    "Output = w0 + w1 * 5 + w2 * 2 = -1 + 2 * 5 + 1 * 2 = 11\n",
    "\n",
    "For (1, -3):\n",
    "Output = w0 + w1 * 1 + w2 * -3 = -1 + 2 * 1 + 1 * -3 = -1\n",
    "\n",
    "For (-8, -3):\n",
    "Output = w0 + w1 * -8 + w2 * -3 = -1 + 2 * -8 + 1 * -3 = -21\n",
    "\n",
    "For (-3, 0):\n",
    "Output = w0 + w1 * -3 + w2 * 0 = -1 + 2 * -3 + 1 * 0 = -7\n",
    "\n",
    "Since the perceptron outputs 1 for values greater than or equal to 0 and 0 for values less than 0, we can classify the data points as follows:\n",
    "\n",
    "(3, 4) and (5, 2) are classified as positive.\n",
    "(1, -3), (-8, -3), and (-3, 0) are classified as negative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba330a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rosenblatt’s perceptron model is a linear binary classifier that receives input features, computes the weighted sum of those features, adds a bias term, and applies a threshold function to produce a binary output. The perceptron is trained using a supervised learning algorithm that adjusts the weights based on the error between the predicted output and the true output. A set of data can be classified using a simple perceptron by iterating over the data points and updating the weights until the perceptron converges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aca2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "An artificial neuron, also known as a perceptron, is a basic unit of an artificial neural network (ANN). It is similar to a biological neuron in that it receives input, processes it, and produces output. The main components of an artificial neuron are:\n",
    "\n",
    "Inputs: These are the signals received from other neurons or the external environment.\n",
    "Weights: Each input signal is multiplied by a weight, which determines the importance of that input to the neuron's output.\n",
    "Bias: This is a constant value that is added to the weighted sum of inputs to shift the activation function.\n",
    "Activation function: This function takes the weighted sum of inputs and bias as input and produces an output. It is typically a non-linear function that introduces non-linearity to the neuron's output.\n",
    "Output: This is the final output of the neuron, which is sent to other neurons in the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "The different types of activation functions popularly used in ANNs are:\n",
    "\n",
    "Sigmoid function: This function produces an output between 0 and 1, and is used in binary classification problems. The output of the sigmoid function can be interpreted as the probability of a particular class.\n",
    "ReLU function: This function produces an output of 0 for negative input values and the input value for positive input values. It is commonly used in deep neural networks and helps to overcome the vanishing gradient problem.\n",
    "Tanh function: This function produces an output between -1 and 1, and is used in classification problems where the output can be negative or positive.\n",
    "Softmax function: This function produces an output between 0 and 1 for each class in a multi-class classification problem. The outputs of the softmax function can be interpreted as the probability of each class.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tExplain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c06cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The backpropagation algorithm used to train a multi-layer feed forward neural network consists of the following steps:\n",
    "\n",
    "Forward propagation: The input is propagated through the network to produce an output.\n",
    "Compute error: The error between the actual output and the desired output is computed.\n",
    "Backward propagation: The error is propagated back through the network, and the weights of the neurons are adjusted to minimize the error.\n",
    "Repeat: The process is repeated for each input in the training set until the error is minimized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tDescribe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1be60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A multi-layer perceptron (MLP) is a neural network that consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. The MLP can solve non-linear classification and regression problems by using non-linear activation functions and adjusting the weights and biases through a training process. The MLP can solve the XOR problem by using a hidden layer with non-linear activation functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba9c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "The backpropagation algorithm is a supervised learning algorithm used to train neural networks. It works by calculating the gradient of the loss function with respect to the weights and biases of the network and updating them to minimize the error. The steps in the backpropagation algorithm are as follows:\n",
    "\n",
    "Forward Pass: The input data is fed forward through the network and the output of each neuron is computed.\n",
    "\n",
    "Calculate Error: The error between the predicted output and the true output is calculated using a loss function.\n",
    "\n",
    "Backward Pass: The error is propagated backward through the network, and the gradient of the loss function with respect to the weights and biases of each neuron is calculated.\n",
    "\n",
    "Weight Update: The weights and biases are updated using an optimization algorithm such as stochastic gradient descent (SGD), which adjusts the weights in the opposite direction of the gradient to minimize the loss.\n",
    "\n",
    "Repeat: The above steps are repeated for a number of epochs or until the error is below a certain threshold.\n",
    "\n",
    "A multi-layer neural network is required because it can model complex non-linear relationships between inputs and outputs. The hidden layers of the network allow it to learn more abstract and complex features of the input data. The backpropagation algorithm can be used to adjust the weights and biases of each neuron in the network, which enables it to learn and generalize from a wide variety of input data. The multi-layer neural network is capable of solving more complex problems such as image recognition, natural language processing, and speech recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWrite short notes on:\n",
    "1.\tArtificial neuron\n",
    "2.\tMulti-layer perceptron\n",
    "3.\tDeep learning\n",
    "4.\tLearning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Artificial neuron: An artificial neuron is a mathematical function that takes inputs, applies a set of weights to them, and produces an output. It is the basic building block of artificial neural networks. An artificial neuron receives input signals from other neurons or external sources, processes them through an activation function, and produces an output signal. The weights are adjusted during the training process to optimize the performance of the network.\n",
    "\n",
    "Multi-layer perceptron: A multi-layer perceptron (MLP) is a type of artificial neural network that consists of multiple layers of neurons. The first layer is the input layer, which receives the input data. The middle layers are called hidden layers, which transform the input into a more abstract representation. The final layer is the output layer, which produces the final output. The weights of the MLP are adjusted during training using the backpropagation algorithm, allowing it to learn complex non-linear relationships between inputs and outputs.\n",
    "\n",
    "Deep learning: Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to learn representations of data. It involves training neural networks with large amounts of data and complex architectures to learn complex patterns and relationships. Deep learning has been used in a wide range of applications, including computer vision, natural language processing, speech recognition, and autonomous vehicles.\n",
    "\n",
    "Learning rate: The learning rate is a hyperparameter that controls how much the weights of a neural network are updated during training. It determines the step size at each iteration while moving towards a minimum of the loss function. A high learning rate can result in the network overshooting the minimum, while a low learning rate can result in slow convergence or getting stuck in local minima. The learning rate is typically set before training and can be adjusted during the training process to optimize the performance of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f89d0c9",
   "metadata": {},
   "source": [
    "#### 11.\tWrite the difference between:-\n",
    "1.\tActivation function vs threshold function\n",
    "2.\tStep function vs sigmoid function\n",
    "3.\tSingle layer vs multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4803b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation function vs threshold function:\n",
    "Activation functions are used in artificial neural networks to introduce non-linearity into the network. They transform the output of a neuron into a desired range. Threshold functions are a type of activation function that produce a binary output of either 0 or 1 based on whether the input is greater than or equal to a certain threshold. The key difference between the two is that activation functions can produce continuous outputs in a desired range, while threshold functions produce discrete outputs of either 0 or 1.\n",
    "\n",
    "Step function vs sigmoid function:\n",
    "Step functions and sigmoid functions are both types of activation functions used in artificial neural networks. Step functions produce a binary output of either 0 or 1 based on whether the input is greater than or equal to a certain threshold. Sigmoid functions produce a continuous output between 0 and 1, based on the input. The key difference between the two is that step functions produce a discontinuous output, while sigmoid functions produce a smooth, continuous output that is useful for gradient-based optimization algorithms.\n",
    "\n",
    "Single layer vs multi-layer perceptron:\n",
    "A single layer perceptron is a type of artificial neural network that consists of a single layer of neurons. It can be used for simple linearly separable problems, such as binary classification. A multi-layer perceptron, on the other hand, consists of multiple layers of neurons, including one or more hidden layers. It is capable of learning complex non-linear relationships between inputs and outputs, making it suitable for a wide range of problems, such as image and speech recognition. The key difference between the two is that a single layer perceptron is limited to solving simple linearly separable problems, while a multi-layer perceptron is capable of solving more complex non-linear problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
